{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to install the ucimlrepo package\n",
    "# !pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "# https://archive.ics.uci.edu/dataset/222/bank+marketing\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "bank_marketing = fetch_ucirepo(id=222)\n",
    "X = bank_marketing.data.features\n",
    "y = bank_marketing.data.targets\n",
    "features_names = X.columns\n",
    "target_name = y.columns[0]\n",
    "print(f\"Shape of features: {X.shape}\")\n",
    "print(f\"Shape of targets: {y.shape}\")\n",
    "print(f\"Features names: {features_names}\")\n",
    "print(f\"Target name: {target_name}\")\n",
    "bank_marketing = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the classes of the target variable\n",
    "for i in y[target_name].unique():\n",
    "    print(f\"Class {i}: {y[target_name].value_counts()[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values of target variable\n",
    "missing_values = y.isnull().sum()\n",
    "print(f\"Missing values of target variable: {missing_values}\")\n",
    "\n",
    "# Count the number of missing values in each column\n",
    "missing_values = X.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns with missing values\n",
    "X = X.dropna(axis=1)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the type of the features\n",
    "print(X.dtypes)\n",
    "# print the type of the target variable\n",
    "print(y.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical variables using one-hot encoding\n",
    "XcatEncoded = pd.get_dummies(X)\n",
    "print(XcatEncoded.columns)\n",
    "# Display the encoded data\n",
    "XcatEncoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into training, validation, and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(XcatEncoded, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "# Create a decision tree classifier\n",
    "# max_depth is the maximum depth of the tree, it varies from 1 to infinity (default=None)\n",
    "# min_samples_leaf is the minimum number of samples required to be at a leaf node, it varies from 1 to infinity (default=2)\n",
    "# ccp_alpha is the complexity parameter for the cost-complexity pruning, it varies from 0.0 to infinity (default=0.0)\n",
    "# criterion is the function used to measure the quality of a split, it varies between 'gini', 'entropy', 'log_loss' (default='gini')\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=2, min_samples_leaf=2, ccp_alpha=0.0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set, and evaluate the classifier\n",
    "y_pred = clf.predict(X_train)\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(f\"Training Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Make predictions on the validation set, and evaluate the classifier\n",
    "y_pred = clf.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "\n",
    "# Make predictions on the test set, and evaluate the classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Depth of the tree: {clf.get_depth()}\")\n",
    "print(f\"Min samples leaf: {clf.min_samples_leaf}\")\n",
    "print(f\"Ccp alpha: {clf.ccp_alpha}\")\n",
    "print(f\"Criterion: {clf.criterion}\")\n",
    "print(f\"Number of leaves of the tree: {clf.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "if clf.get_n_leaves() < 10:\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plot_tree(clf, filled=True, feature_names=XcatEncoded.columns, \n",
    "            class_names=np.unique(y.values), fontsize=10)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"The tree is too large to visualize, number of leaves: \", clf.get_n_leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "tree_rules = export_text(clf, feature_names=list(XcatEncoded.columns))\n",
    "print(tree_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOF_FOLDS = 5 # Number of folds\n",
    "PERCENTAGE_TEST = 0.2 # Percentage of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of classes in the target variable\n",
    "print(f\"Number of instances in the target variable: {y.value_counts()}\")\n",
    "print(f\"Percentage of classes in the target variable: {y.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(XcatEncoded, \n",
    "                                                    y, \n",
    "                                                    test_size=PERCENTAGE_TEST, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, perform cross-validation on the training set\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=NOF_FOLDS, shuffle=True, random_state=42)\n",
    "average_accuracy_train = 0\n",
    "average_accuracy_val = 0\n",
    "for fold_index, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Print the number of instances per class in the training and validation folds\n",
    "    print(f\"Number of instances per class in the training fold: {y_train_fold.value_counts()}\")\n",
    "    print(f\"Percentage of instances per class in the training fold: {y_train_fold.value_counts(normalize=True)}\")\n",
    "    print(f\"Number of instances per class in the validation fold: {y_val_fold.value_counts()}\")\n",
    "    print(f\"Percentage of instances per class in the validation fold: {y_val_fold.value_counts(normalize=True)}\")\n",
    "\n",
    "    clf = DecisionTreeClassifier(criterion='gini', max_depth=5, min_samples_leaf=2, ccp_alpha=0.01)\n",
    "    clf.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_train = clf.predict(X_train_fold)\n",
    "    accuracy_train = accuracy_score(y_train_fold, y_pred_train)\n",
    "    print(f\"Training Accuracy: {accuracy_train:.5f}, fold {fold_index+1} of {NOF_FOLDS}\")\n",
    "    y_pred_val = clf.predict(X_val_fold)\n",
    "    accuracy_val = accuracy_score(y_val_fold, y_pred_val)\n",
    "    print(f\"Validation Accuracy: {accuracy_val:.5f}, fold {fold_index+1} of {NOF_FOLDS}\")\n",
    "    print(100*\"-\")\n",
    "\n",
    "    average_accuracy_train += accuracy_train\n",
    "    average_accuracy_val += accuracy_val\n",
    "\n",
    "print(100*\"-\")\n",
    "average_accuracy_train /= NOF_FOLDS\n",
    "print(f\"Average Training Accuracy: {average_accuracy_train:.5f}\")\n",
    "average_accuracy_val /= NOF_FOLDS\n",
    "print(f\"Average Cross-validation Accuracy: {average_accuracy_val:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOF_FOLDS = 5\n",
    "PERCENTAGE_TEST = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split the data into train and test sets, using stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(XcatEncoded, \n",
    "                                                    y, \n",
    "                                                    test_size=PERCENTAGE_TEST, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of classes in the target variable\n",
    "print(f\"Number of instances in the training set: {y_train.value_counts()}\")\n",
    "print(f\"Percentage of classes in the training set: {y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Number of instances in the test set: {y_test.value_counts()}\")\n",
    "print(f\"Percentage of classes in the test set: {y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, perform stratified cross-validation on the training set\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kf = StratifiedKFold(n_splits=NOF_FOLDS, shuffle=True, random_state=42)\n",
    "average_accuracy_train = 0\n",
    "average_accuracy_val = 0\n",
    "for fold_index, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Print the number of instances per class in the training and validation folds\n",
    "    print(f\"Number of instances per class in the training fold: {y_train_fold.value_counts()}\")\n",
    "    print(f\"Number of instances per class in the validation fold: {y_val_fold.value_counts()}\")\n",
    "\n",
    "    clf = DecisionTreeClassifier(criterion='gini', max_depth=5, min_samples_leaf=2, ccp_alpha=0.01)\n",
    "    clf.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_train = clf.predict(X_train_fold)\n",
    "    accuracy_train = accuracy_score(y_train_fold, y_pred_train)\n",
    "    print(f\"Training Accuracy: {accuracy_train:.5f}, fold {fold_index+1} of {NOF_FOLDS}\")\n",
    "    y_pred_val = clf.predict(X_val_fold)\n",
    "    accuracy_val = accuracy_score(y_val_fold, y_pred_val)\n",
    "    print(f\"Validation Accuracy: {accuracy_val:.5f}, fold {fold_index+1} of {NOF_FOLDS}\")\n",
    "    print(100*\"-\")\n",
    "\n",
    "    average_accuracy_train += accuracy_train\n",
    "    average_accuracy_val += accuracy_val\n",
    "\n",
    "print(100*\"-\")\n",
    "average_accuracy_train /= NOF_FOLDS\n",
    "print(f\"Average Training Accuracy: {average_accuracy_train:.5f}\")\n",
    "average_accuracy_val /= NOF_FOLDS\n",
    "print(f\"Average Cross-validation Accuracy: {average_accuracy_val:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code3133",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
