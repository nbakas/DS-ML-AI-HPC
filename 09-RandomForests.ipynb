{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from numpy import sum, corrcoef, zeros\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by preparing some synthetic data $Xtr, ytr$. \n",
    "\n",
    "The $ytr$ is the summation of the $Xtr$, plus a random noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = 1_000\n",
    "vars = 10\n",
    "rng = default_rng(seed=0)\n",
    "Xtr = rng.random((obs, vars))\n",
    "ytr = sum(Xtr, axis=1) + (rng.random(obs)-1/2)\n",
    "Xte = rng.random((obs, vars))\n",
    "yte = sum(Xte, axis=1) + (rng.random(obs)-1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with the training of a simple Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeRegressor(max_depth=10, random_state=0)\n",
    "dtree.fit(Xtr, ytr)\n",
    "\n",
    "# Accuracy\n",
    "pred_tr = dtree.predict(Xtr)\n",
    "acc_tr = corrcoef(ytr,pred_tr)[0,1]\n",
    "print(acc_tr)\n",
    "pred_te = dtree.predict(Xte)\n",
    "acc_te = corrcoef(yte,pred_te)[0,1]\n",
    "print(acc_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a high accuracy in the Train Set, but low in the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with Random Forests. We set the number of trees, and create some random sub-sampling of the columns. We store the indices of the columns in $inds\\_cols$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nof_trees = 100\n",
    "# we need to keep inds_cols, for train AND later for prediction\n",
    "nof_cols = int(0.7*vars)\n",
    "inds_cols = zeros((nof_trees,nof_cols),dtype=int)\n",
    "for j in range(nof_trees):\n",
    "    rng = default_rng(seed=j+1)\n",
    "    jj = rng.integers(low=0, high=vars, size=nof_cols)\n",
    "    inds_cols[j,:] = jj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we check if the columns have been sampled uniformly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_cols_vec = inds_cols.reshape(nof_trees*nof_cols,1)[:,0]\n",
    "inds_cols_vec\n",
    "counter = Counter(inds_cols_vec)\n",
    "vals = counter.values()\n",
    "keys = counter.keys()\n",
    "plt.bar(keys,vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with the Training of the Random Forest. For each random sampling of the columns that we did previously, we train a decision tree. Furthermore, for each tree, we randomly sub-sampling the rows of the dataset. We store all trained trees in the list $all\\_trees$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nof_rows = int(0.7*obs)\n",
    "all_trees = []\n",
    "for j in range(nof_trees):\n",
    "    dtree = DecisionTreeRegressor(max_depth=10, random_state=j+1)\n",
    "    rng = default_rng(seed=j+1)\n",
    "    ii = rng.integers(low=0, high=obs, size=nof_rows)\n",
    "    dtree.fit(Xtr[ii,:][:,inds_cols[j,:]], ytr[ii])\n",
    "    all_trees.append(dtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we predict for the train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tr = zeros(obs)\n",
    "pred_te = zeros(obs)\n",
    "for j in range(nof_trees):\n",
    "    j_dtree = all_trees[j]\n",
    "    pred_tr += j_dtree.predict(Xtr[:,inds_cols[j,:]])\n",
    "    pred_te += j_dtree.predict(Xte[:,inds_cols[j,:]])\n",
    "pred_tr /= nof_trees\n",
    "pred_te /= nof_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_tr = corrcoef(ytr,pred_tr)[0,1]\n",
    "print(acc_tr)\n",
    "acc_te = corrcoef(yte,pred_te)[0,1]\n",
    "print(acc_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy has been vastly improved, and it is similar for the train and test sets, indicating generalization capabilities without over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train a Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=nof_trees, max_depth=10, random_state=0)\n",
    "rf.fit(Xtr, ytr)\n",
    "\n",
    "# Predict for train and test sets\n",
    "pred_tr_rf = rf.predict(Xtr)\n",
    "pred_te_rf = rf.predict(Xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Function to compute the Pearson Correlation Coefficient\n",
    "def pearson_correlation(y_true, y_pred):\n",
    "    return np.corrcoef(y_true, y_pred)[0, 1]\n",
    "\n",
    "# Function to compute MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "# Function to compute MAXAPE\n",
    "def max_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.max(np.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "# Function to compute MAMPE\n",
    "def mean_absolute_mean_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred)) / np.mean(y_true)\n",
    "\n",
    "# Function to compute slope and intercept using least squares\n",
    "def compute_slope_intercept(y_true, y_pred):\n",
    "    # vstack is used to stack the arrays in sequence vertically (row-wise)\n",
    "    A = np.vstack([y_true, np.ones(len(y_true))]).T\n",
    "    # linalg.lstsq is used to solve the linear least squares problem. It returns the intercept and slope of the line that best fits the data.\n",
    "    slope, intercept = np.linalg.lstsq(A, y_pred, rcond=None)[0]\n",
    "    return slope, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for train and test sets\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"Pearson Correlation\": pearson_correlation(y_true, y_pred),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"MAPE\": mean_absolute_percentage_error(y_true, y_pred),\n",
    "        \"MAXAPE\": max_absolute_percentage_error(y_true, y_pred),\n",
    "        \"MAMPE\": mean_absolute_mean_percentage_error(y_true, y_pred),\n",
    "        \"Slope\": compute_slope_intercept(y_true, y_pred)[0],\n",
    "        \"Intercept\": compute_slope_intercept(y_true, y_pred)[1]\n",
    "    }# This is a Python dictionary, which is a data structure used to store key-value pairs.\n",
    "    # Each key (e.g., \"Pearson Correlation\", \"RMSE\", etc.) is associated with a value, which is computed using functions like pearson_correlation(y_true, y_pred) and mean_squared_error(y_true, y_pred).\n",
    "\n",
    "metrics_train = compute_metrics(ytr, pred_tr_rf)\n",
    "metrics_test = compute_metrics(yte, pred_te_rf)\n",
    "\n",
    "import pandas as pd\n",
    "# We create a pandas DataFrame to store the metrics for train and test sets.\n",
    "metrics_df = pd.DataFrame({\"Train\": metrics_train, \"Test\": metrics_test}).T\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot target versus predicted for train set\n",
    "plt.figure(figsize=(6, 5))  # Create a new figure with a specific size\n",
    "plt.scatter(ytr, pred_tr_rf, alpha=0.5)  # Scatter plot of actual vs predicted train values with transparency\n",
    "# Extract slope, intercept, and Pearson correlation for the train set\n",
    "slope_train, intercept_train, pearson_train = metrics_train[\"Slope\"], metrics_train[\"Intercept\"], metrics_train[\"Pearson Correlation\"]\n",
    "# Plot the trend line based on the slope and intercept\n",
    "plt.plot([0, ytr.max()], [intercept_train, slope_train * ytr.max() + intercept_train], 'r-', lw=2, label=f'Trend line (slope={slope_train:.2f}, intercept={intercept_train:.2f}, Pearson={pearson_train:.2f})')\n",
    "# Plot the ideal line where predicted values equal actual values\n",
    "plt.plot([0, ytr.max()], [0, ytr.max()], 'k--', lw=2)\n",
    "plt.legend()  # Add a legend to the plot\n",
    "plt.xlabel('Actual Train Values')  # Label for the x-axis\n",
    "plt.ylabel('Predicted Train Values')  # Label for the y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot target versus predicted for test set\n",
    "plt.figure(figsize=(6, 5))  # Create a new figure with a specific size\n",
    "plt.scatter(yte, pred_te_rf, alpha=0.5)  # Scatter plot of actual vs predicted test values with transparency\n",
    "# Extract slope, intercept, and Pearson correlation for the test set\n",
    "slope_test, intercept_test, pearson_test = metrics_test[\"Slope\"], metrics_test[\"Intercept\"], metrics_test[\"Pearson Correlation\"]\n",
    "# Plot the trend line based on the slope and intercept\n",
    "plt.plot([0, yte.max()], [intercept_test, slope_test * yte.max() + intercept_test], 'r-', lw=2, label=f'Trend line (slope={slope_test:.2f}, intercept={intercept_test:.2f}, Pearson={pearson_test:.2f})')\n",
    "# Plot the ideal line where predicted values equal actual values\n",
    "plt.plot([0, yte.max()], [0, yte.max()], 'k--', lw=2)\n",
    "plt.legend()  # Add a legend to the plot\n",
    "plt.xlabel('Actual Test Values')  # Label for the x-axis\n",
    "plt.ylabel('Predicted Test Values')  # Label for the y-axis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
