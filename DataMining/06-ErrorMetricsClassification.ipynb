{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_classification will create a dataset with 1000 samples, 20 features, 2 classes, and a random state of 42\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets with a 70-30 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Decision Tree Classifier with specified hyperparameters\n",
    "model = DecisionTreeClassifier(criterion='gini', max_depth=5, min_samples_leaf=10, ccp_alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class probabilities for the test set. We\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_proba[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix and extract true negatives, false positives, false negatives, and true positives\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(f'TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate various performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)  # Overall accuracy of the model\n",
    "precision = precision_score(y_test, y_pred)  # Precision or positive predictive value\n",
    "recall = recall_score(y_test, y_pred)  # Recall or sensitivity\n",
    "f1 = f1_score(y_test, y_pred)  # F1 Score, the harmonic mean of precision and recall\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "unique_scores = np.unique(y_proba)\n",
    "unique_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)  # False Positive Rate and True Positive Rate\n",
    "print(f\"The thresholds are: {thresholds}\")\n",
    "print(f\"The false positive rate is: {fpr}\")\n",
    "print(f\"The true positive rate is: {tpr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC (Area Under the Curve)\n",
    "roc_auc = auc(fpr, tpr)  # Calculate the AUC\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC Curve\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')  # Plot the ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Plot a diagonal line for reference\n",
    "plt.scatter(fpr, tpr, color='red')  # Add points to the ROC curve for better visualization\n",
    "for i, txt in enumerate(thresholds):\n",
    "    plt.annotate(f'Threshold: {txt:.2f}', (fpr[i], tpr[i]), fontsize=8)  # Add threshold numbers on points\n",
    "plt.xlabel('False Positive Rate')  # Label for the x-axis\n",
    "plt.ylabel('True Positive Rate')  # Label for the y-axis\n",
    "plt.title('ROC Curve')  # Title of the plot\n",
    "plt.legend()  # Add a legend to the plot\n",
    "plt.show()  # Display the plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
