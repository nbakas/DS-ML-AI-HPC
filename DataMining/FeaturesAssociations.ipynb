{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to install the ucimlrepo package\n",
    "# !pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas is a well-known python library for data manipulation and analysis\n",
    "# It is used to load, and analyze data ususally stored in dataframes\n",
    "# The dataframes are tables with rows and columns\n",
    "# The rows are called samples or observations\n",
    "# The columns are called features or variables\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://archive.ics.uci.edu/dataset/222/bank+marketing\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "bank_marketing = fetch_ucirepo(id=222) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = bank_marketing.data.features \n",
    "y = bank_marketing.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(bank_marketing.metadata) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable information \n",
    "print(bank_marketing.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the features\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the targets\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 5 random rows of the features\n",
    "X.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 5 random rows of the targets\n",
    "y.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features names\n",
    "feature_names = X.columns\n",
    "# Target names\n",
    "target_name = y.columns\n",
    "# Display the feature names\n",
    "print(feature_names)\n",
    "# Display the target names\n",
    "print(target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"subscribe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with missing values\n",
    "missing_values = X.isnull().sum()\n",
    "# Display the columns with missing values\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the columns with missing values\n",
    "X = X.dropna(axis=1)\n",
    "# Display the columns with missing values\n",
    "missing_values = X.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive statistics metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate descriptive statistics for numerical features\n",
    "numerical_stats = X.describe()\n",
    "print(\"Descriptive statistics for numerical features:\")\n",
    "print(numerical_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate descriptive statistics for categorical features\n",
    "# include=['O'] is used to include only the categorical features\n",
    "categorical_stats = X.describe(include=['O'])\n",
    "# Print a message indicating that the descriptive statistics for categorical features will be displayed\n",
    "print(\"\\nDescriptive statistics for categorical features:\")\n",
    "# Print the descriptive statistics for categorical features\n",
    "print(categorical_stats)\n",
    "\n",
    "# Calculate and display the number of instances for each class in the categorical features\n",
    "for column in X.select_dtypes(include=['O']).columns:\n",
    "    print(f\"\\nNumber of instances per class for the categorical feature '{column}':\")\n",
    "    class_counts = X[column].value_counts()\n",
    "    print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive statistics plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the numerical data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Plot histograms for numerical features\n",
    "for column in X.select_dtypes(include=[np.number]).columns:\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.histplot(X[column], kde=True)\n",
    "    plt.title(f'Histogram of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the categorical data\n",
    "# Visualize the categorical data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot bar plots for categorical features\n",
    "for column in X.select_dtypes(include=['O']).columns:\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.countplot(x=column, data=X, order=X[column].value_counts().index)\n",
    "    plt.title(f'Bar plot of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical variables using one-hot encoding\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Display the encoded features and target\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature names are the column names of the encoded features\n",
    "feature_names = X.columns\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable\n",
    "y_encoded = y['y'].map({'yes': 1, 'no': 0})\n",
    "print(y_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encoded features to a numpy array\n",
    "X = X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the true/false values to floats\n",
    "X = X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associations all by all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for linear regression and correlation calculation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Define a function to plot numeric vs numeric data\n",
    "def plot_numeric_numeric(xx, yy, xx_name, yy_name, plot_figure=False):\n",
    "    # Sort the data based on xx values\n",
    "    iso = np.argsort(xx)\n",
    "    xx = xx[iso]\n",
    "    yy = yy[iso]\n",
    "    \n",
    "    # Initialize and fit a linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(xx.reshape(-1, 1), yy)\n",
    "    y_pred = model.predict(xx.reshape(-1, 1))\n",
    "    \n",
    "    # Calculate Pearson correlation coefficient\n",
    "    pearson_corr, _ = pearsonr(xx, yy)\n",
    "    \n",
    "    # Plot the figure if plot_figure is True\n",
    "    if plot_figure: \n",
    "        # Keep only 1000 points equally spaced for plotting\n",
    "        xx = xx[::len(xx)//1000]\n",
    "        yy = yy[::len(yy)//1000]\n",
    "        y_pred = y_pred[::len(y_pred)//1000]\n",
    "        \n",
    "        # Calculate R-squared value\n",
    "        r_squared = model.score(xx.reshape(-1, 1), yy)\n",
    "        \n",
    "        # Prepare the plot title with correlation and regression details\n",
    "        plot_title = f\"Pearson Correlation={pearson_corr:.5f}, R-Squared={r_squared:.5f}, \"    \n",
    "        print(plot_title + f\"{yy_name}={model.coef_[0]:.2f}({xx_name})+{model.intercept_:.2f}\")\n",
    "        \n",
    "        # Create a plotly figure\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=xx, y=yy, mode='markers', name=f'Given Data Points of {xx_name} vs {yy_name}'))\n",
    "        fig.add_trace(go.Scatter(x=xx, y=y_pred, mode='lines+markers', name='Trend Line', marker=dict(color='grey')))\n",
    "        \n",
    "        # Update layout of the figure\n",
    "        fig.update_layout(title=plot_title, xaxis_title=xx_name, yaxis_title=yy_name, template='plotly_white', \n",
    "                          legend=dict(yanchor=\"top\", y=1.05, xanchor=\"left\", x=0.01), width=500, height=400, font=dict(size=10))   \n",
    "        fig.show()\n",
    "    \n",
    "    # Return the Pearson correlation coefficient\n",
    "    return pearson_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for logistic regression and evaluation metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "# Define a function to plot numeric vs categorical data\n",
    "def plot_numeric_categorical(xx, yy, xx_name, yy_name, plot_figure=False):\n",
    "    # Sort the data based on xx values\n",
    "    iso = np.argsort(xx)\n",
    "    xx = xx[iso]\n",
    "    yy = yy[iso]\n",
    "    \n",
    "    # Initialize and fit a logistic regression model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(xx.reshape(-1, 1), yy)\n",
    "    \n",
    "    # Predict the binary outcomes and probabilities\n",
    "    y_pred = model.predict(xx.reshape(-1, 1))\n",
    "    y_pred_proba = model.predict_proba(xx.reshape(-1, 1))[:, 1]\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    accuracy1 = f1_score(yy, y_pred)\n",
    "\n",
    "    # Plot the figure if plot_figure is True\n",
    "    if plot_figure:\n",
    "        # Keep only 1000 points equally spaced for plotting\n",
    "        xx = xx[::len(xx)//1000]\n",
    "        yy = yy[::len(yy)//1000]\n",
    "        y_pred_proba = y_pred_proba[::len(y_pred_proba)//1000]\n",
    "        y_pred = y_pred[::len(y_pred)//1000]\n",
    "        \n",
    "        # Prepare the plot title with evaluation metrics\n",
    "        plot_title = f\"F1 Score={accuracy1:.2f}, Accuracy={accuracy_score(yy, y_pred):.2f}, AUC={roc_auc_score(yy, y_pred_proba):.2f}\"\n",
    "        print(plot_title + f\", {yy_name}=1/(1+exp(-({model.coef_[0][0]:.2f}({xx_name}) + {model.intercept_[0]:.2f})))\")\n",
    "        \n",
    "        # Create a plotly figure\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=xx, y=yy, mode='markers', name='Given Data Points'))\n",
    "        fig.add_trace(go.Scatter(x=xx, y=y_pred_proba, mode='lines+markers', name='Logistic Regression Predictions', \n",
    "                        marker=dict(color='grey')))\n",
    "        fig.add_trace(go.Scatter(x=xx, y=y_pred, mode='markers', name='Logistic Regression Predictions (binary)'))\n",
    "        \n",
    "        # Update layout of the figure\n",
    "        fig.update_layout(title=plot_title, xaxis_title=xx_name, yaxis_title=yy_name, template='plotly_white', width=500, height=400, font=dict(size=10))\n",
    "        fig.show()\n",
    "    \n",
    "    # Return the F1 score\n",
    "    return accuracy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_categorical_numeric(xx, yy, xx_name, yy_name, plot_figure=False):\n",
    "    groups_anova = [yy[xx == groupAnv] for groupAnv in np.unique(xx)]  # Split yy by xx groups\n",
    "    ss_total = np.sum((yy - np.mean(yy)) ** 2)  # Total sum of squares\n",
    "    ss_between = np.sum([len(groupAnv) * (np.mean(groupAnv) - np.mean(yy)) ** 2 for groupAnv in groups_anova])  # Between-group sum of squares\n",
    "    # anova_eta_squared is the ratio of the between-group sum of squares to the total sum of squares.\n",
    "    # It measures the proportion of the total variance in the dependent variable that is explained by the independent variable.\n",
    "    # The closer the value is to 1, the stronger the relationship between the independent variable and the dependent variable.\n",
    "    anova_eta_squared = ss_between / ss_total\n",
    "    if plot_figure:\n",
    "        # Get unique classes from xx\n",
    "        unique_classes = np.unique(xx)\n",
    "        \n",
    "        # Create the plot title\n",
    "        my_title = f\"Histograms of {yy_name} for each class of {xx_name}\"\n",
    "        print(my_title + f\", Eta Squared={anova_eta_squared:.2f}\")\n",
    "        \n",
    "        # Initialize a plotly figure\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Add histogram traces for each unique class\n",
    "        for cls in unique_classes:\n",
    "            fig.add_trace(go.Histogram(x=yy[xx == cls], name=f'{xx_name}={cls}', opacity=0.75))\n",
    "        \n",
    "        # Update the layout of the figure\n",
    "        fig.update_layout(barmode='overlay', title=my_title, \n",
    "                          xaxis_title=yy_name, yaxis_title='Count', template='plotly_white', width=500, height=400, font=dict(size=10))\n",
    "        \n",
    "        # Display the figure\n",
    "        fig.show()\n",
    "        \n",
    "    return anova_eta_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import plotly.express as px\n",
    "def plot_categorical_categorical(xx, yy, xx_name, yy_name, plot_figure=False):\n",
    "    # Convert input arrays to boolean arrays for logical operations\n",
    "    xx_bool = np.array(xx, dtype=bool)\n",
    "    yy_bool = np.array(yy, dtype=bool)\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    my_precision = np.sum(xx_bool & yy_bool) / np.sum(xx_bool) if np.sum(xx_bool) > 0 else 0\n",
    "    my_recall = np.sum(xx_bool & yy_bool) / np.sum(yy_bool) if np.sum(yy_bool) > 0 else 0\n",
    "    my_f1_score = 2 * (my_precision * my_recall) / (my_precision + my_recall) if (my_precision + my_recall) > 0 else 0\n",
    "    \n",
    "    if plot_figure:\n",
    "        # Generate confusion matrix\n",
    "        conf_matrix = confusion_matrix(xx, yy)\n",
    "        \n",
    "        # Create labels for the confusion matrix axes\n",
    "        labels_xy = sorted(set(xx) | set(yy))  # Unique labels from both xx and yy\n",
    "        labels_x = [f\"{xx_name}={str(int(label))}\" for label in labels_xy]\n",
    "        labels_y = [f\"{yy_name}={str(int(label))}\" for label in labels_xy]\n",
    "\n",
    "        # Set the title for the plot\n",
    "        my_title = f\"Confusion Matrix of {xx_name} vs {yy_name}\"\n",
    "        print(my_title + f\", F1 Score={my_f1_score:.2f}, Accuracy={accuracy_score(xx, yy):.2f}, AUC={roc_auc_score(xx, yy):.2f}\")\n",
    "        \n",
    "        # Create and display the plot using Plotly\n",
    "        fig = px.imshow(conf_matrix, title=my_title, \n",
    "                        x=labels_x, y=labels_y, text_auto=True, color_continuous_scale='Blues')\n",
    "        fig.update_layout(font=dict(size=10), width=500, height=400)\n",
    "        fig.show()\n",
    "    \n",
    "    # Return the F1 score and its label\n",
    "    return my_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_feature_categorical = []  # Initialize an empty list to store whether each feature is categorical\n",
    "for i in range(X.shape[1]):  # Iterate over each feature in the dataset\n",
    "    if np.unique(X[:, i]).size > 2:  # Check if the feature has more than 2 unique values\n",
    "        is_feature_categorical.append(False)  # If yes, it is not categorical\n",
    "    else:\n",
    "        is_feature_categorical.append(True)  # If no, it is categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store different types of correlations\n",
    "all_numeric_numeric_correlations = []\n",
    "all_numeric_categorical_correlations = []\n",
    "all_categorical_categorical_correlations = []\n",
    "all_categorical_numeric_correlations = []\n",
    "\n",
    "# Iterate over each feature pair in the dataset\n",
    "all_by_all_associations = np.zeros((X.shape[1], X.shape[1]))\n",
    "for i in range(X.shape[1]):\n",
    "    for j in range(X.shape[1]):\n",
    "        if i != j:  # Ensure we are not comparing the same feature\n",
    "            # Check if both features are numeric\n",
    "            if not is_feature_categorical[i] and not is_feature_categorical[j]:\n",
    "                pearson_corr = plot_numeric_numeric(X[:, i], X[:, j], feature_names[i], feature_names[j], plot_figure=False)\n",
    "                print(f\"Pearson correlation between {feature_names[i]} and {feature_names[j]} is {pearson_corr:.5f}\")\n",
    "                all_numeric_numeric_correlations.append((i, j, pearson_corr))\n",
    "                all_by_all_associations[i, j] = pearson_corr\n",
    "            # Check if the first feature is numeric and the second is categorical\n",
    "            elif not is_feature_categorical[i] and is_feature_categorical[j]:\n",
    "                anova_eta_squared = plot_numeric_categorical(X[:, i], X[:, j], feature_names[i], feature_names[j], plot_figure=False)\n",
    "                print(f\"ANOVA Eta Squared between {feature_names[i]} and {feature_names[j]} is {anova_eta_squared:.5f}\")\n",
    "                all_numeric_categorical_correlations.append((i, j, anova_eta_squared))\n",
    "                all_by_all_associations[i, j] = anova_eta_squared\n",
    "            # Check if both features are categorical\n",
    "            elif is_feature_categorical[i] and is_feature_categorical[j]:\n",
    "                my_f1_score = plot_categorical_categorical(X[:, i], X[:, j], feature_names[i], feature_names[j], plot_figure=False)\n",
    "                print(f\"F1 Score between {feature_names[i]} and {feature_names[j]} is {my_f1_score:.5f}\")\n",
    "                all_categorical_categorical_correlations.append((i, j, my_f1_score))\n",
    "                all_by_all_associations[i, j] = my_f1_score\n",
    "            # Check if the first feature is categorical and the second is numeric\n",
    "            elif is_feature_categorical[i] and not is_feature_categorical[j]:\n",
    "                anova_eta_squared = plot_categorical_numeric(X[:, i], X[:, j], feature_names[i], feature_names[j], plot_figure=False)\n",
    "                print(f\"ANOVA Eta Squared between {feature_names[i]} and {feature_names[j]} is {anova_eta_squared:.5f}\")\n",
    "                all_categorical_numeric_correlations.append((i, j, anova_eta_squared))\n",
    "                all_by_all_associations[i, j] = anova_eta_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(all_by_all_associations, annot=True, fmt=\".2f\", cmap=\"coolwarm\", xticklabels=feature_names, yticklabels=feature_names)\n",
    "plt.title(\"Feature Associations Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top numeric-numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_numeric_numeric_correlations = sorted(all_numeric_numeric_correlations, key=lambda x: x[2], reverse=True)\n",
    "top_numeric_numeric_correlations[:3]\n",
    "for i, j, corr in top_numeric_numeric_correlations[:3]:\n",
    "    plot_numeric_numeric(X[:, i], X[:, j], feature_names[i], feature_names[j], plot_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top numeric-categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_numeric_categorical_correlations = sorted(all_numeric_categorical_correlations, key=lambda x: x[2], reverse=True)\n",
    "top_numeric_categorical_correlations[:3]\n",
    "for i, j, corr in top_numeric_categorical_correlations[:3]:\n",
    "    plot_numeric_categorical(X[:, i], X[:, j], feature_names[i], feature_names[j], plot_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top categorical-categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_categorical_categorical_correlations = sorted(all_categorical_categorical_correlations, key=lambda x: x[2], reverse=True)\n",
    "top_categorical_categorical_correlations[:3]\n",
    "for i, j, corr in top_categorical_categorical_correlations[:3]:\n",
    "    plot_categorical_categorical(X[:, i], X[:, j], feature_names[i], feature_names[j], plot_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top categorical-numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_categorical_numeric_correlations = sorted(all_categorical_numeric_correlations, key=lambda x: x[2], reverse=True)\n",
    "top_categorical_numeric_correlations[:3]\n",
    "for i, j, corr in top_categorical_numeric_correlations[:3]:\n",
    "    plot_categorical_numeric(X[:, i], X[:, j], feature_names[i], feature_names[j], plot_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "# max_depth is the maximum depth of the tree\n",
    "clf = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(clf, filled=True, feature_names=feature_names, \n",
    "          class_names=np.unique(y.values), fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "tree_rules = export_text(clf, feature_names=feature_names)\n",
    "print(tree_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importances\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "import pandas as pd\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the feature importances\n",
    "print(feature_importances_df)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importances_df['Feature'], feature_importances_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
